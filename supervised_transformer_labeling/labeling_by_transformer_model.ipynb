{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ff62ff",
   "metadata": {},
   "source": [
    "# Labeling the dataset by an transformer model\n",
    "In this notebook, you are going to use the pre-trained transformer model, which is fine-tuned by the `transfomers_model_finetuner.py` script, to label the `labeled.csv` dataset. The first labeling method only labels a random sample of 700 unlabelled data points, while the second function labels the entire dataset.\n",
    "## Pre-setup\n",
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16092940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d4741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"../models/t5_E5_balanced_25_06/hf_transformer_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c78f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading the model from ../models/t5_E5_balanced_25_06/hf_transformer_model ---\n",
      "Model and tokenizer loaded successfully!\n",
      "Model architecture: T5ForSequenceClassification(\n",
      "  (transformer): T5Model(\n",
      "    (shared): Embedding(32128, 512)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 8)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-5): 5 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (decoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 8)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-5): 5 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classification_head): T5ClassificationHead(\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (out_proj): Linear(in_features=512, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Loading the model from {model_dir} ---\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    load_device = torch.device(\"mps\")\n",
    "else:\n",
    "    load_device = torch.device(\"cpu\")\n",
    "\n",
    "loaded_model.to(load_device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "print(\"Model architecture:\", loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd795d2",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca44278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/jz38hmzn5nn2wyvvkc86zngc0000gn/T/ipykernel_3224/3490284572.py:1: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv('../../data/labeled.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72984"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/labeled.csv')\n",
    "dataset = dataset[dataset['class'] == 'unknown']\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2152af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_to_class = {\n",
    "    1: 'animal welfare',\n",
    "    2: 'blm',\n",
    "    3: 'climate',\n",
    "    4: 'culture',\n",
    "    5: 'discrimination',\n",
    "    6: 'education',\n",
    "    7: 'environment',\n",
    "    8: 'farmers',\n",
    "    9: 'health care',\n",
    "    10: 'housing',\n",
    "    11: 'immigration',\n",
    "    12: 'labor rights',\n",
    "    13: 'lgbtq',\n",
    "    14: 'palestine-israel conflict',\n",
    "    15: 'pandemic',\n",
    "    16: 'policies & politics',\n",
    "    17: 'public services',\n",
    "    18: 'ukraine-russia war',\n",
    "    19: 'unjust law enforcement',\n",
    "    20: 'women rights',\n",
    "\n",
    "}\n",
    "\n",
    "class_to_index = {v: k for k, v in Index_to_class.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c0781",
   "metadata": {},
   "source": [
    "## Labeling on 700 unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee44618",
   "metadata": {},
   "outputs": [],
   "source": [
    "unkown_data = []\n",
    "random_indexes = random.sample(range(len(dataset)), 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a581f",
   "metadata": {},
   "source": [
    "Change the last line to save it to another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c0a78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m inputs = {key: val.to(load_device) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     outputs = loaded_model(**inputs)\n\u001b[32m     10\u001b[39m     logits = outputs.logits\n\u001b[32m     11\u001b[39m     predicted_class = torch.argmax(logits, dim=\u001b[32m1\u001b[39m).item() + \u001b[32m1\u001b[39m  \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:2180\u001b[39m, in \u001b[36mT5ForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   2173\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2174\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf no `decoder_input_ids` or `decoder_inputs_embeds` are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2175\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpassed, `input_ids` cannot be `None`. Please pass either \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2176\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2177\u001b[39m         )\n\u001b[32m   2178\u001b[39m     decoder_input_ids = \u001b[38;5;28mself\u001b[39m._shift_right(input_ids)\n\u001b[32m-> \u001b[39m\u001b[32m2180\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.transformer(\n\u001b[32m   2181\u001b[39m     input_ids,\n\u001b[32m   2182\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   2183\u001b[39m     decoder_input_ids=decoder_input_ids,\n\u001b[32m   2184\u001b[39m     decoder_attention_mask=decoder_attention_mask,\n\u001b[32m   2185\u001b[39m     head_mask=head_mask,\n\u001b[32m   2186\u001b[39m     decoder_head_mask=decoder_head_mask,\n\u001b[32m   2187\u001b[39m     cross_attn_head_mask=cross_attn_head_mask,\n\u001b[32m   2188\u001b[39m     encoder_outputs=encoder_outputs,\n\u001b[32m   2189\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m   2190\u001b[39m     decoder_inputs_embeds=decoder_inputs_embeds,\n\u001b[32m   2191\u001b[39m     use_cache=use_cache,\n\u001b[32m   2192\u001b[39m     output_attentions=output_attentions,\n\u001b[32m   2193\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m   2194\u001b[39m     return_dict=return_dict,\n\u001b[32m   2195\u001b[39m )\n\u001b[32m   2196\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   2198\u001b[39m eos_mask = input_ids.eq(\u001b[38;5;28mself\u001b[39m.config.eos_token_id).to(sequence_output.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1648\u001b[39m, in \u001b[36mT5Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1646\u001b[39m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[32m   1647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m     encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m   1649\u001b[39m         input_ids=input_ids,\n\u001b[32m   1650\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   1651\u001b[39m         inputs_embeds=inputs_embeds,\n\u001b[32m   1652\u001b[39m         head_mask=head_mask,\n\u001b[32m   1653\u001b[39m         output_attentions=output_attentions,\n\u001b[32m   1654\u001b[39m         output_hidden_states=output_hidden_states,\n\u001b[32m   1655\u001b[39m         return_dict=return_dict,\n\u001b[32m   1656\u001b[39m     )\n\u001b[32m   1657\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[32m   1658\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1659\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1660\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1661\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1662\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1131\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1114\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1115\u001b[39m         layer_module.forward,\n\u001b[32m   1116\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1128\u001b[39m         cache_position,\n\u001b[32m   1129\u001b[39m     )\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     layer_outputs = layer_module(\n\u001b[32m   1132\u001b[39m         hidden_states,\n\u001b[32m   1133\u001b[39m         attention_mask=causal_mask,\n\u001b[32m   1134\u001b[39m         position_bias=position_bias,\n\u001b[32m   1135\u001b[39m         encoder_hidden_states=encoder_hidden_states,\n\u001b[32m   1136\u001b[39m         encoder_attention_mask=encoder_extended_attention_mask,\n\u001b[32m   1137\u001b[39m         encoder_decoder_position_bias=encoder_decoder_position_bias,\n\u001b[32m   1138\u001b[39m         layer_head_mask=layer_head_mask,\n\u001b[32m   1139\u001b[39m         cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n\u001b[32m   1140\u001b[39m         past_key_value=past_key_values,\n\u001b[32m   1141\u001b[39m         use_cache=use_cache,\n\u001b[32m   1142\u001b[39m         output_attentions=output_attentions,\n\u001b[32m   1143\u001b[39m         return_dict=return_dict,\n\u001b[32m   1144\u001b[39m         cache_position=cache_position,\n\u001b[32m   1145\u001b[39m     )\n\u001b[32m   1147\u001b[39m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[32m   1148\u001b[39m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:682\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    667\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    668\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    680\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    681\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     self_attention_outputs = \u001b[38;5;28mself\u001b[39m.layer[\u001b[32m0\u001b[39m](\n\u001b[32m    683\u001b[39m         hidden_states,\n\u001b[32m    684\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    685\u001b[39m         position_bias=position_bias,\n\u001b[32m    686\u001b[39m         layer_head_mask=layer_head_mask,\n\u001b[32m    687\u001b[39m         past_key_value=past_key_value,\n\u001b[32m    688\u001b[39m         use_cache=use_cache,\n\u001b[32m    689\u001b[39m         output_attentions=output_attentions,\n\u001b[32m    690\u001b[39m         cache_position=cache_position,\n\u001b[32m    691\u001b[39m     )\n\u001b[32m    692\u001b[39m     hidden_states, past_key_value = self_attention_outputs[:\u001b[32m2\u001b[39m]\n\u001b[32m    693\u001b[39m     attention_outputs = self_attention_outputs[\u001b[32m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:599\u001b[39m, in \u001b[36mT5LayerSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    589\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    590\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    597\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     normed_hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m    600\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.SelfAttention(\n\u001b[32m    601\u001b[39m         normed_hidden_states,\n\u001b[32m    602\u001b[39m         mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    608\u001b[39m         cache_position=cache_position,\n\u001b[32m    609\u001b[39m     )\n\u001b[32m    610\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(attention_output[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:260\u001b[39m, in \u001b[36mT5LayerNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    257\u001b[39m hidden_states = hidden_states * torch.rsqrt(variance + \u001b[38;5;28mself\u001b[39m.variance_epsilon)\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight.dtype \u001b[38;5;129;01min\u001b[39;00m [torch.float16, torch.bfloat16]:\n\u001b[32m    261\u001b[39m     hidden_states = hidden_states.to(\u001b[38;5;28mself\u001b[39m.weight.dtype)\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight * hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1918\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1909\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1911\u001b[39m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[32m   1912\u001b[39m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[32m   1913\u001b[39m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1916\u001b[39m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[32m   1917\u001b[39m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m   1919\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1920\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "MAX_LEN_HF = 128\n",
    "for i in random_indexes:\n",
    "    text = dataset['clean_notes'].iloc[i]\n",
    "    onfiltert_text = dataset['notes'].iloc[i]\n",
    "    inputs = loaded_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LEN_HF)\n",
    "    inputs = {key: val.to(load_device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item() + 1  \n",
    "\n",
    "    unkown_data.append([onfiltert_text, Index_to_class[predicted_class], dataset['class'].iloc[i]])\n",
    "\n",
    "new_df = pd.DataFrame(unkown_data, columns=['notes', 'class', 'orginal_class'])\n",
    "new_df.to_csv('../../data/KINBERT_3000.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729645e",
   "metadata": {},
   "source": [
    "## Labeling over the hole dataset\n",
    "Change the last line to save it to another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01a6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN_HF = 128\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    text = dataset['clean_notes'].iloc[i]\n",
    "    inputs = loaded_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LEN_HF)\n",
    "    inputs = {key: val.to(load_device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item() + 1  \n",
    "\n",
    "    dataset.at[i, 'predicted_class'] = Index_to_class[predicted_class]\n",
    "dataset.to_csv('../../data/filtered_events_class_with_predicted.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
