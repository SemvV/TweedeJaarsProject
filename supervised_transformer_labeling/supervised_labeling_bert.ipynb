{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4284349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275bb92",
   "metadata": {},
   "source": [
    "# Hugging Face (Transformers) BERT Sequence Classification\n",
    "This notebook utilizes the Hugging Face Transformers library to perform sequence classification using a pretrained bidirectional transformer on unlabeled data. One such model we use is called BERT. BERT is employed as both a tokenizer and a sequence classification model. This notebook also makes heavy use of `AutoTokenizer` and `AutoModel` to easily integrate the BERT model for this classification task.\n",
    "\n",
    "## Detailed information\n",
    "\n",
    "### BERT\n",
    "In this notebook the `bert-bases-uncases` with 110M parameters from google is used. [MORE INFROMATION IS COMMING] \n",
    "- 12 transformer block layers\n",
    "- Hidden size of 768\n",
    "- linear layer and softmax\n",
    "\n",
    "![Alt text for the image](https://www.researchgate.net/publication/374608193/figure/fig2/AS:11431281210596149@1702055674618/BERT-base-uncased-model-architecture-which-comprises-12-transformer-block-layers-each.tif)\n",
    "\n",
    "Documentation\n",
    "- https://huggingface.co/google-bert/bert-base-uncased \n",
    "- https://huggingface.co/docs/transformers/en/model_doc/bert\n",
    "\n",
    "### AutoTokenizer and Automodel\n",
    "[MORE INFROMATION IS COMMING] \n",
    "\n",
    "Documentation\n",
    "- https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
    "\n",
    "## Load and Format Data\n",
    "This dataset is from the AG-news dataset, which contains `Descriptions` and classifications for 5 different types of news called `Class Index` column. The dataset is being used for early testing until the project's main dataset is ready. We going to drop the `Title` column, because we are not using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8245b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                        Description\n",
       "0            3  Reuters - Short-sellers, Wall Street's dwindli...\n",
       "1            3  Reuters - Private investment firm Carlyle Grou...\n",
       "2            3  Reuters - Soaring crude prices plus worries\\ab...\n",
       "3            3  Reuters - Authorities have halted oil export\\f...\n",
       "4            3  AFP - Tearaway world oil prices, toppling reco..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df = pd.read_csv('train.csv')\n",
    "#test_df = pd.read_csv('test.csv')\n",
    "\n",
    "#train_df = train_df.drop(['Title'], axis=1)\n",
    "#test_df = test_df.drop(['Title'], axis=1)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60c203b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "environment                             6000\n",
      "Culture                                 6000\n",
      "Education                               6000\n",
      "Palestine-Israel Conflict               6000\n",
      "Labor Rights                            6000\n",
      "Public Services & Social Welfare        6000\n",
      "Justice & Civil Rights                  6000\n",
      "Climate Action & Animal Welfare         6000\n",
      "Political & Democratic Governance       5911\n",
      "Ukraine-Russia War                      5587\n",
      "Infrastructure                          5427\n",
      "Climate Action & Resource Management    4404\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/filtered_events_class.csv')\n",
    "#dataset = dataset[['class', 'clean_notes']]\n",
    "\n",
    "# Remove all rows with an class name NoN\n",
    "#dataset = dataset[dataset['class'] != 'NoN']\n",
    "\n",
    "# print classes types\n",
    "print(dataset['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c70ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69329,\n",
       " 55463,\n",
       " 13866,\n",
       " class\n",
       " 6     4830\n",
       " 5     4815\n",
       " 1     4807\n",
       " 7     4803\n",
       " 8     4795\n",
       " 4     4791\n",
       " 2     4775\n",
       " 3     4759\n",
       " 9     4699\n",
       " 10    4513\n",
       " 11    4323\n",
       " 12    3553\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Index_to_class = {\n",
    "    1: 'environment',\n",
    "    2: 'Culture',\n",
    "    3: 'Education',\n",
    "    4: 'Palestine-Israel Conflict',\n",
    "    5: 'Labor Rights',\n",
    "    6: 'Public Services & Social Welfare',\n",
    "    7: 'Justice & Civil Rights',\n",
    "    8: 'Climate Action & Animal Welfare',\n",
    "    9: 'Political & Democratic Governance',\n",
    "    10: 'Ukraine-Russia War',\n",
    "    11: 'Infrastructure',\n",
    "    12: 'Climate Action & Resource Management',\n",
    "\n",
    "}\n",
    "\n",
    "class_to_index = {v: k for k, v in Index_to_class.items()}\n",
    "\n",
    "dataset['class'] = dataset['class'].map(class_to_index)\n",
    "dataset['class'] = dataset['class'].astype(int)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_df = dataset.sample(frac=0.8, random_state=42)\n",
    "test_df = dataset.drop(train_df.index)\n",
    "class_counts = train_df['class'].value_counts()\n",
    "\n",
    "\n",
    "len(dataset), len(train_df) , len(test_df), class_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c493f",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9729f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face Tokenizer - First training input_ids: tensor([  101,  5958,  2925,  7423,  7645,  4957, 17686,  9808,  3334,  3995,\n",
      "        19270,  4009,  3086,  4483,  3277,  5157,  2895,  4785,  2689,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Hugging Face Tokenizer - First training attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Corresponding training label: tensor(7)\n",
      "Max sequence length (HF): 128\n",
      "Vocabulary size (HF): 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "MAX_LEN_HF = 128\n",
    "\n",
    "def tokenize_function(texts, tokenizer, max_len):\n",
    "    return tokenizer(texts,\n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=max_len,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "\n",
    "train_encodings = tokenize_function(train_df['clean_notes'].tolist(), tokenizer, MAX_LEN_HF)\n",
    "test_encodings = tokenize_function(test_df['clean_notes'].tolist(), tokenizer, MAX_LEN_HF)\n",
    "\n",
    "\n",
    "y_train_hf = torch.tensor((train_df['class'] - 1).values, dtype=torch.long)\n",
    "y_test_hf = torch.tensor((test_df['class'] - 1).values, dtype=torch.long)\n",
    "\n",
    "print(\"\\nHugging Face Tokenizer - First training input_ids:\", train_encodings['input_ids'][0])\n",
    "print(\"Hugging Face Tokenizer - First training attention_mask:\", train_encodings['attention_mask'][0])\n",
    "print(\"Corresponding training label:\", y_train_hf[0])\n",
    "print(f\"Max sequence length (HF): {MAX_LEN_HF}\")\n",
    "print(f\"Vocabulary size (HF): {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2bce1",
   "metadata": {},
   "source": [
    "### Creating Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26f8e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face Dataset - First item: {'input_ids': tensor([  101,  5958,  2925,  7423,  7645,  4957, 17686,  9808,  3334,  3995,\n",
      "        19270,  4009,  3086,  4483,  3277,  5157,  2895,  4785,  2689,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(7)}\n"
     ]
    }
   ],
   "source": [
    "class AGNewsDatasetHF(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset_hf = AGNewsDatasetHF(train_encodings, y_train_hf)\n",
    "test_dataset_hf = AGNewsDatasetHF(test_encodings, y_test_hf)\n",
    "\n",
    "print(\"\\nHugging Face Dataset - First item:\", train_dataset_hf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face DataLoader - Number of batches in train_loader: 3467\n",
      "Hugging Face DataLoader - Keys in batch: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "Hugging Face DataLoader - Shape of input_ids: torch.Size([16, 128])\n",
      "Hugging Face DataLoader - Shape of labels: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader_hf = DataLoader(train_dataset_hf, batch_size=batch_size, shuffle=True)\n",
    "test_loader_hf = DataLoader(test_dataset_hf, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nHugging Face DataLoader - Number of batches in train_loader: {len(train_loader_hf)}\")\n",
    "for batch in train_loader_hf:\n",
    "    print(\"Hugging Face DataLoader - Keys in batch:\", batch.keys())\n",
    "    print(\"Hugging Face DataLoader - Shape of input_ids:\", batch['input_ids'].shape)\n",
    "    print(\"Hugging Face DataLoader - Shape of labels:\", batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ff747",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97542aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face Transformer Model:\n",
      " BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_hf = AutoModelForSequenceClassification.from_pretrained(tokenizer_name, num_labels=16)\n",
    "print(\"\\nHugging Face Transformer Model:\\n\", model_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51295d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "057db176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for Hugging Face Transformer Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3467/3467 [3:18:08<00:00,  3.43s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 0.1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3467/3467 [5:29:33<00:00,  5.70s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Training Loss: 0.0742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3467/3467 [35:24<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Average Training Loss: 0.0598\n",
      "Hugging Face Transformer Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model_hf.to(device)\n",
    "\n",
    "learning_rate_hf = 5e-5\n",
    "num_epochs_hf = 3\n",
    "\n",
    "optimizer_hf = AdamW(model_hf.parameters(), lr=learning_rate_hf)\n",
    "\n",
    "print(\"\\nStarting training for Hugging Face Transformer Model...\")\n",
    "for epoch in range(num_epochs_hf):\n",
    "    model_hf.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader_hf, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        optimizer_hf.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model_hf(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_hf.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader_hf)\n",
    "    print(f\"Epoch {epoch+1}: Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "print(\"Hugging Face Transformer Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b763c17",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17a11c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 867/867 [02:33<00:00,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Transformer Test Accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_hf.eval()\n",
    "correct_predictions_hf = 0\n",
    "total_predictions_hf = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_hf, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model_hf(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total_predictions_hf += labels.size(0)\n",
    "        correct_predictions_hf += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_hf = correct_predictions_hf / total_predictions_hf\n",
    "print(f\"Hugging Face Transformer Test Accuracy: {accuracy_hf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbf88d",
   "metadata": {},
   "source": [
    "## Manage the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae08066",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../models/t5_E5_24_06/hf_transformer_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e0307",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e3bacc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./ag_news_model_saved\n",
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "model_hf.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Model and tokenizer saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10200950",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd830b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading the model from ../models/t5_E5_24_06/hf_transformer_model ---\n",
      "Model and tokenizer loaded successfully!\n",
      "Model architecture: T5ForSequenceClassification(\n",
      "  (transformer): T5Model(\n",
      "    (shared): Embedding(32128, 512)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 8)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-5): 5 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (decoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 8)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-5): 5 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classification_head): T5ClassificationHead(\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (out_proj): Linear(in_features=512, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Loading the model from {output_dir} ---\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    load_device = torch.device(\"mps\")\n",
    "else:\n",
    "    load_device = torch.device(\"cpu\")\n",
    "\n",
    "loaded_model.to(load_device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "print(\"Model architecture:\", loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "386b14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/jz38hmzn5nn2wyvvkc86zngc0000gn/T/ipykernel_8138/2993241431.py:7: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv('../../data/labeled.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72984"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = pd.read_csv('../../data/filtered_events_class.csv')\n",
    "#dataset = dataset[['class', 'clean_notes']]\n",
    "\n",
    "# Remove all rows with an class name NoN\n",
    "#dataset = dataset[dataset['class'] == 'unknown']\n",
    "\n",
    "dataset = pd.read_csv('../../data/labeled.csv')\n",
    "#dataset = dataset[['class', 'clean_notes']]\n",
    "\n",
    "# Remove all rows with an class name NoN\n",
    "dataset = dataset[dataset['class'] == 'unknown']\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95142d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3648152",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_to_class = {\n",
    "    1: 'animal welfare',\n",
    "    2: 'blm',\n",
    "    3: 'climate',\n",
    "    4: 'culture',\n",
    "    5: 'discrimination',\n",
    "    6: 'education',\n",
    "    7: 'environment',\n",
    "    8: 'farmers',\n",
    "    9: 'health care',\n",
    "    10: 'housing',\n",
    "    11: 'immigration',\n",
    "    12: 'labor rights',\n",
    "    13: 'lgbtq',\n",
    "    14: 'palestine-israel conflict',\n",
    "    15: 'pandemic',\n",
    "    16: 'policies & politics',\n",
    "    17: 'public services',\n",
    "    18: 'ukraine-russia war',\n",
    "    19: 'unjust law enforcement',\n",
    "    20: 'women rights',\n",
    "\n",
    "}\n",
    "\n",
    "class_to_index = {v: k for k, v in Index_to_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac3cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unkown_data = []\n",
    "\n",
    "# an list with 700 randmom indexes from the dataset\n",
    "import random\n",
    "random_indexes = random.sample(range(len(dataset)), 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7178423",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_HF = 128\n",
    "for i in random_indexes:\n",
    "    text = dataset['clean_notes'].iloc[i]\n",
    "    onfiltert_text = dataset['notes'].iloc[i]\n",
    "    inputs = loaded_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LEN_HF)\n",
    "    inputs = {key: val.to(load_device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item() + 1  \n",
    "\n",
    "    unkown_data.append([onfiltert_text, Index_to_class[predicted_class], dataset['class'].iloc[i]])\n",
    "    #print(f\"Text: {onfiltert_text}\\nPredicted Class Index: {predicted_class}, Class Name: {Index_to_class[predicted_class]}\\n\")\n",
    "\n",
    "new_df = pd.DataFrame(unkown_data, columns=['notes', 'class', 'orginal_class'])\n",
    "new_df.to_csv('../../data/unknows_labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4cc299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/jz38hmzn5nn2wyvvkc86zngc0000gn/T/ipykernel_9952/4222626780.py:1: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv('../../data/filtered_events_class.csv')\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/filtered_events_class.csv')\n",
    "\n",
    "# Add an column with the class name for predicated classes\n",
    "dataset['predicted_class'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1086e8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id_cnty</th>\n",
       "      <th>event_date</th>\n",
       "      <th>year</th>\n",
       "      <th>time_precision</th>\n",
       "      <th>disorder_type</th>\n",
       "      <th>event_type</th>\n",
       "      <th>sub_event_type</th>\n",
       "      <th>actor1</th>\n",
       "      <th>assoc_actor_1</th>\n",
       "      <th>inter1</th>\n",
       "      <th>...</th>\n",
       "      <th>source_scale</th>\n",
       "      <th>notes</th>\n",
       "      <th>fatalities</th>\n",
       "      <th>tags</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>population_best</th>\n",
       "      <th>clean_notes</th>\n",
       "      <th>country_code</th>\n",
       "      <th>class</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEL4179</td>\n",
       "      <td>23 May 2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>Demonstrations</td>\n",
       "      <td>Protests</td>\n",
       "      <td>Peaceful protest</td>\n",
       "      <td>Protesters (Belgium)</td>\n",
       "      <td>Government of Belgium (2025-); Green</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>Subnational</td>\n",
       "      <td>On 23 May 2025, in the afternoon, at the call ...</td>\n",
       "      <td>0</td>\n",
       "      <td>crowd size=several</td>\n",
       "      <td>1748377529</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>afternoon call green west flanders groen west ...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>environment</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BEL4183</td>\n",
       "      <td>23 May 2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>Demonstrations</td>\n",
       "      <td>Protests</td>\n",
       "      <td>Peaceful protest</td>\n",
       "      <td>Protesters (Belgium)</td>\n",
       "      <td>XR: Extinction Rebellion</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>Other-National</td>\n",
       "      <td>On 23 May 2025, XR protested at the Farmers Un...</td>\n",
       "      <td>0</td>\n",
       "      <td>crowd size=no report</td>\n",
       "      <td>1748377529</td>\n",
       "      <td>10978.0</td>\n",
       "      <td>xr protested farmer union bb office leuven vla...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>farmers</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BGR4378</td>\n",
       "      <td>23 May 2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>Demonstrations</td>\n",
       "      <td>Protests</td>\n",
       "      <td>Peaceful protest</td>\n",
       "      <td>Protesters (Bulgaria)</td>\n",
       "      <td>Labor Group (Bulgaria)</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>National</td>\n",
       "      <td>On 23 May 2025, librarians and representatives...</td>\n",
       "      <td>0</td>\n",
       "      <td>crowd size=no report</td>\n",
       "      <td>1748377530</td>\n",
       "      <td>194.0</td>\n",
       "      <td>librarian representative cultural institution ...</td>\n",
       "      <td>BGR</td>\n",
       "      <td>labor rights</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGR4379</td>\n",
       "      <td>23 May 2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>Demonstrations</td>\n",
       "      <td>Protests</td>\n",
       "      <td>Peaceful protest</td>\n",
       "      <td>Protesters (Bulgaria)</td>\n",
       "      <td>Labor Group (Bulgaria)</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>National</td>\n",
       "      <td>On 23 May 2025, librarians and representatives...</td>\n",
       "      <td>0</td>\n",
       "      <td>crowd size=no report</td>\n",
       "      <td>1748377530</td>\n",
       "      <td>7324.0</td>\n",
       "      <td>librarian representative cultural institution ...</td>\n",
       "      <td>BGR</td>\n",
       "      <td>labor rights</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BGR4380</td>\n",
       "      <td>23 May 2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>Demonstrations</td>\n",
       "      <td>Protests</td>\n",
       "      <td>Peaceful protest</td>\n",
       "      <td>Protesters (Bulgaria)</td>\n",
       "      <td>Labor Group (Bulgaria)</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>National</td>\n",
       "      <td>On 23 May 2025, librarians and representatives...</td>\n",
       "      <td>0</td>\n",
       "      <td>crowd size=no report</td>\n",
       "      <td>1748377530</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>librarian representative cultural institution ...</td>\n",
       "      <td>BGR</td>\n",
       "      <td>labor rights</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  event_id_cnty   event_date  year  time_precision   disorder_type event_type  \\\n",
       "0       BEL4179  23 May 2025  2025               1  Demonstrations   Protests   \n",
       "1       BEL4183  23 May 2025  2025               1  Demonstrations   Protests   \n",
       "2       BGR4378  23 May 2025  2025               1  Demonstrations   Protests   \n",
       "3       BGR4379  23 May 2025  2025               1  Demonstrations   Protests   \n",
       "4       BGR4380  23 May 2025  2025               1  Demonstrations   Protests   \n",
       "\n",
       "     sub_event_type                 actor1  \\\n",
       "0  Peaceful protest   Protesters (Belgium)   \n",
       "1  Peaceful protest   Protesters (Belgium)   \n",
       "2  Peaceful protest  Protesters (Bulgaria)   \n",
       "3  Peaceful protest  Protesters (Bulgaria)   \n",
       "4  Peaceful protest  Protesters (Bulgaria)   \n",
       "\n",
       "                          assoc_actor_1  inter1  ...    source_scale  \\\n",
       "0  Government of Belgium (2025-); Green       6  ...     Subnational   \n",
       "1              XR: Extinction Rebellion       6  ...  Other-National   \n",
       "2                Labor Group (Bulgaria)       6  ...        National   \n",
       "3                Labor Group (Bulgaria)       6  ...        National   \n",
       "4                Labor Group (Bulgaria)       6  ...        National   \n",
       "\n",
       "                                               notes  fatalities  \\\n",
       "0  On 23 May 2025, in the afternoon, at the call ...           0   \n",
       "1  On 23 May 2025, XR protested at the Farmers Un...           0   \n",
       "2  On 23 May 2025, librarians and representatives...           0   \n",
       "3  On 23 May 2025, librarians and representatives...           0   \n",
       "4  On 23 May 2025, librarians and representatives...           0   \n",
       "\n",
       "                   tags   timestamp population_best  \\\n",
       "0    crowd size=several  1748377529          2753.0   \n",
       "1  crowd size=no report  1748377529         10978.0   \n",
       "2  crowd size=no report  1748377530           194.0   \n",
       "3  crowd size=no report  1748377530          7324.0   \n",
       "4  crowd size=no report  1748377530          1502.0   \n",
       "\n",
       "                                         clean_notes country_code  \\\n",
       "0  afternoon call green west flanders groen west ...          BEL   \n",
       "1  xr protested farmer union bb office leuven vla...          BEL   \n",
       "2  librarian representative cultural institution ...          BGR   \n",
       "3  librarian representative cultural institution ...          BGR   \n",
       "4  librarian representative cultural institution ...          BGR   \n",
       "\n",
       "          class  predicted_class  \n",
       "0   environment                   \n",
       "1       farmers                   \n",
       "2  labor rights                   \n",
       "3  labor rights                   \n",
       "4  labor rights                   \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a4683ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    text = dataset['clean_notes'].iloc[i]\n",
    "    inputs = loaded_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LEN_HF)\n",
    "    inputs = {key: val.to(load_device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item() + 1  \n",
    "\n",
    "    dataset.at[i, 'predicted_class'] = Index_to_class[predicted_class]\n",
    "dataset.to_csv('../../data/filtered_events_class_with_predicted.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53404bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
