{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4284349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275bb92",
   "metadata": {},
   "source": [
    "# Hugging Face (Transformers) BERT Sequence Classification\n",
    "This notebook utilizes the Hugging Face Transformers library to perform sequence classification using a pretrained bidirectional transformer on unlabeled data. One such model we use is called BERT. BERT is employed as both a tokenizer and a sequence classification model. This notebook also makes heavy use of `AutoTokenizer` and `AutoModel` to easily integrate the BERT model for this classification task.\n",
    "\n",
    "## Detailed information\n",
    "\n",
    "### BERT\n",
    "In this notebook the `bert-bases-uncases` with 110M parameters from google is used. [MORE INFROMATION IS COMMING] \n",
    "- 12 transformer block layers\n",
    "- Hidden size of 768\n",
    "- linear layer and softmax\n",
    "\n",
    "![Alt text for the image](https://www.researchgate.net/publication/374608193/figure/fig2/AS:11431281210596149@1702055674618/BERT-base-uncased-model-architecture-which-comprises-12-transformer-block-layers-each.tif)\n",
    "\n",
    "Documentation\n",
    "- https://huggingface.co/google-bert/bert-base-uncased \n",
    "- https://huggingface.co/docs/transformers/en/model_doc/bert\n",
    "\n",
    "### AutoTokenizer and Automodel\n",
    "[MORE INFROMATION IS COMMING] \n",
    "\n",
    "Documentation\n",
    "- https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
    "\n",
    "## Load and Format Data\n",
    "This dataset is from the AG-news dataset, which contains `Descriptions` and classifications for 5 different types of news called `Class Index` column. The dataset is being used for early testing until the project's main dataset is ready. We going to drop the `Title` column, because we are not using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8245b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                        Description\n",
       "0            3  Reuters - Short-sellers, Wall Street's dwindli...\n",
       "1            3  Reuters - Private investment firm Carlyle Grou...\n",
       "2            3  Reuters - Soaring crude prices plus worries\\ab...\n",
       "3            3  Reuters - Authorities have halted oil export\\f...\n",
       "4            3  AFP - Tearaway world oil prices, toppling reco..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df = pd.read_csv('train.csv')\n",
    "#test_df = pd.read_csv('test.csv')\n",
    "\n",
    "#train_df = train_df.drop(['Title'], axis=1)\n",
    "#test_df = test_df.drop(['Title'], axis=1)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60c203b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "environment                             6000\n",
      "Culture                                 6000\n",
      "Education                               6000\n",
      "Palestine-Israel Conflict               6000\n",
      "Labor Rights                            6000\n",
      "Public Services & Social Welfare        6000\n",
      "Justice & Civil Rights                  6000\n",
      "Climate Action & Animal Welfare         6000\n",
      "Political & Democratic Governance       5911\n",
      "Ukraine-Russia War                      5587\n",
      "Infrastructure                          5427\n",
      "Climate Action & Resource Management    4404\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/filtered_events_class.csv')\n",
    "#dataset = dataset[['class', 'clean_notes']]\n",
    "\n",
    "# Remove all rows with an class name NoN\n",
    "#dataset = dataset[dataset['class'] != 'NoN']\n",
    "\n",
    "# print classes types\n",
    "print(dataset['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c70ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69329,\n",
       " 55463,\n",
       " 13866,\n",
       " class\n",
       " 6     4830\n",
       " 5     4815\n",
       " 1     4807\n",
       " 7     4803\n",
       " 8     4795\n",
       " 4     4791\n",
       " 2     4775\n",
       " 3     4759\n",
       " 9     4699\n",
       " 10    4513\n",
       " 11    4323\n",
       " 12    3553\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Index_to_class = {\n",
    "    1: 'environment',\n",
    "    2: 'Culture',\n",
    "    3: 'Education',\n",
    "    4: 'Palestine-Israel Conflict',\n",
    "    5: 'Labor Rights',\n",
    "    6: 'Public Services & Social Welfare',\n",
    "    7: 'Justice & Civil Rights',\n",
    "    8: 'Climate Action & Animal Welfare',\n",
    "    9: 'Political & Democratic Governance',\n",
    "    10: 'Ukraine-Russia War',\n",
    "    11: 'Infrastructure',\n",
    "    12: 'Climate Action & Resource Management',\n",
    "\n",
    "}\n",
    "\n",
    "class_to_index = {v: k for k, v in Index_to_class.items()}\n",
    "\n",
    "dataset['class'] = dataset['class'].map(class_to_index)\n",
    "dataset['class'] = dataset['class'].astype(int)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_df = dataset.sample(frac=0.8, random_state=42)\n",
    "test_df = dataset.drop(train_df.index)\n",
    "class_counts = train_df['class'].value_counts()\n",
    "\n",
    "\n",
    "len(dataset), len(train_df) , len(test_df), class_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c493f",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9729f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face Tokenizer - First training input_ids: tensor([  101,  5958,  2925,  7423,  7645,  4957, 17686,  9808,  3334,  3995,\n",
      "        19270,  4009,  3086,  4483,  3277,  5157,  2895,  4785,  2689,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Hugging Face Tokenizer - First training attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Corresponding training label: tensor(7)\n",
      "Max sequence length (HF): 128\n",
      "Vocabulary size (HF): 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "MAX_LEN_HF = 128\n",
    "\n",
    "def tokenize_function(texts, tokenizer, max_len):\n",
    "    return tokenizer(texts,\n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=max_len,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "\n",
    "train_encodings = tokenize_function(train_df['clean_notes'].tolist(), tokenizer, MAX_LEN_HF)\n",
    "test_encodings = tokenize_function(test_df['clean_notes'].tolist(), tokenizer, MAX_LEN_HF)\n",
    "\n",
    "\n",
    "y_train_hf = torch.tensor((train_df['class'] - 1).values, dtype=torch.long)\n",
    "y_test_hf = torch.tensor((test_df['class'] - 1).values, dtype=torch.long)\n",
    "\n",
    "print(\"\\nHugging Face Tokenizer - First training input_ids:\", train_encodings['input_ids'][0])\n",
    "print(\"Hugging Face Tokenizer - First training attention_mask:\", train_encodings['attention_mask'][0])\n",
    "print(\"Corresponding training label:\", y_train_hf[0])\n",
    "print(f\"Max sequence length (HF): {MAX_LEN_HF}\")\n",
    "print(f\"Vocabulary size (HF): {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2bce1",
   "metadata": {},
   "source": [
    "### Creating Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26f8e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face Dataset - First item: {'input_ids': tensor([  101,  5958,  2925,  7423,  7645,  4957, 17686,  9808,  3334,  3995,\n",
      "        19270,  4009,  3086,  4483,  3277,  5157,  2895,  4785,  2689,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(7)}\n"
     ]
    }
   ],
   "source": [
    "class AGNewsDatasetHF(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset_hf = AGNewsDatasetHF(train_encodings, y_train_hf)\n",
    "test_dataset_hf = AGNewsDatasetHF(test_encodings, y_test_hf)\n",
    "\n",
    "print(\"\\nHugging Face Dataset - First item:\", train_dataset_hf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face DataLoader - Number of batches in train_loader: 3467\n",
      "Hugging Face DataLoader - Keys in batch: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "Hugging Face DataLoader - Shape of input_ids: torch.Size([16, 128])\n",
      "Hugging Face DataLoader - Shape of labels: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader_hf = DataLoader(train_dataset_hf, batch_size=batch_size, shuffle=True)\n",
    "test_loader_hf = DataLoader(test_dataset_hf, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nHugging Face DataLoader - Number of batches in train_loader: {len(train_loader_hf)}\")\n",
    "for batch in train_loader_hf:\n",
    "    print(\"Hugging Face DataLoader - Keys in batch:\", batch.keys())\n",
    "    print(\"Hugging Face DataLoader - Shape of input_ids:\", batch['input_ids'].shape)\n",
    "    print(\"Hugging Face DataLoader - Shape of labels:\", batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ff747",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97542aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face Transformer Model:\n",
      " BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_hf = AutoModelForSequenceClassification.from_pretrained(tokenizer_name, num_labels=16)\n",
    "print(\"\\nHugging Face Transformer Model:\\n\", model_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51295d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "057db176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for Hugging Face Transformer Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 3467/3467 [3:18:08<00:00,  3.43s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 0.1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 3467/3467 [5:29:33<00:00,  5.70s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Training Loss: 0.0742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 3467/3467 [35:24<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Average Training Loss: 0.0598\n",
      "Hugging Face Transformer Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model_hf.to(device)\n",
    "\n",
    "learning_rate_hf = 5e-5\n",
    "num_epochs_hf = 3\n",
    "\n",
    "optimizer_hf = AdamW(model_hf.parameters(), lr=learning_rate_hf)\n",
    "\n",
    "print(\"\\nStarting training for Hugging Face Transformer Model...\")\n",
    "for epoch in range(num_epochs_hf):\n",
    "    model_hf.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader_hf, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        optimizer_hf.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model_hf(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_hf.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader_hf)\n",
    "    print(f\"Epoch {epoch+1}: Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "print(\"Hugging Face Transformer Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b763c17",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17a11c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 867/867 [02:33<00:00,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Transformer Test Accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_hf.eval()\n",
    "correct_predictions_hf = 0\n",
    "total_predictions_hf = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_hf, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model_hf(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total_predictions_hf += labels.size(0)\n",
    "        correct_predictions_hf += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_hf = correct_predictions_hf / total_predictions_hf\n",
    "print(f\"Hugging Face Transformer Test Accuracy: {accuracy_hf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbf88d",
   "metadata": {},
   "source": [
    "## Manage the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae08066",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/models/T5_small_5E/hf_transformer_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e0307",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e3bacc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./ag_news_model_saved\n",
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "model_hf.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Model and tokenizer saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10200950",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd830b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading the model from ../data/models/T5_small_5E/hf_transformer_model ---\n",
      "Model and tokenizer loaded successfully!\n",
      "Model architecture: T5ForSequenceClassification(\n",
      "  (transformer): T5Model(\n",
      "    (shared): Embedding(32128, 512)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 8)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-5): 5 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (decoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 8)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-5): 5 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classification_head): T5ClassificationHead(\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (out_proj): Linear(in_features=512, out_features=19, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Loading the model from {output_dir} ---\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    load_device = torch.device(\"mps\")\n",
    "else:\n",
    "    load_device = torch.device(\"cpu\")\n",
    "\n",
    "loaded_model.to(load_device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "print(\"Model architecture:\", loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386b14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/jz38hmzn5nn2wyvvkc86zngc0000gn/T/ipykernel_3057/3467126314.py:1: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv('../data/filtered_events_class.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "183080"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/filtered_events_class.csv')\n",
    "#dataset = dataset[['class', 'clean_notes']]\n",
    "\n",
    "# Remove all rows with an class name NoN\n",
    "dataset = dataset[dataset['class'] == 'unknown']\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3648152",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_to_class = {\n",
    "    1: 'animal welfare',\n",
    "    2: 'climate',\n",
    "    3: 'culture',\n",
    "    4: 'discrimination',\n",
    "    5: 'education',\n",
    "    6: 'environment',\n",
    "    7: 'farmers',\n",
    "    8: 'health care',\n",
    "    9: 'housing',\n",
    "    10: 'immigration',\n",
    "    11: 'labor rights',\n",
    "    12: 'lgbtq',\n",
    "    13: 'palestine-israel conflict',\n",
    "    14: 'pandemic',\n",
    "    15: 'policies',\n",
    "    16: 'public services',\n",
    "    17: 'ukraine-russia war',\n",
    "    18: 'unjust law enforcement',\n",
    "    19: 'women rights',\n",
    "\n",
    "}\n",
    "\n",
    "class_to_index = {v: k for k, v in Index_to_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac3cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unkown_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7178423",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_HF = 128\n",
    "for i in range(len(dataset)):\n",
    "    text = dataset['clean_notes'].iloc[i]\n",
    "    onfiltert_text = dataset['notes'].iloc[i]\n",
    "    inputs = loaded_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LEN_HF)\n",
    "    inputs = {key: val.to(load_device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item() + 1  \n",
    "\n",
    "    unkown_data.append([onfiltert_text, Index_to_class[predicted_class]])\n",
    "    #print(f\"Text: {onfiltert_text}\\nPredicted Class Index: {predicted_class}, Class Name: {Index_to_class[predicted_class]}\\n\")\n",
    "\n",
    "new_df = pd.DataFrame(unkown_data, columns=['notes', 'class'])\n",
    "new_df.to_csv('../data/unknows_labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc299a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
